# -*- coding: utf-8 -*-
"""TB-Aware GPT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/166MSk-z8l9M_QuEBW0rTgLnjr6r_dLDJ
"""

import os
from getpass import getpass

# Set up OpenAI API key
groq_api_key = getpass("Enter your Groq key: ")
os.environ["GROQ_API_KEY"] = groq_api_key

!pip install -q langchain openai chromadb tiktoken pypdf langchain_openai

!pip install -q langchain-community

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

!pip install langchain-groq langchain duckduckgo-search langchain_community ddgs  --quiet

from langchain_groq import ChatGroq
from langchain.memory import ConversationBufferWindowMemory

llm = ChatGroq(model="llama-3.1-8b-instant", temperature=0.0, max_tokens=1024)
memory = ConversationBufferWindowMemory(memory_key="chat_history", return_messages=True, k=5)

loader = PyPDFLoader("/content/Guidance-Document-on-TB-Mukt-Bharat-Abhiyan_0.pdf")
docs = loader.load()
splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
chunks = splitter.split_documents(docs)

embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"  # lightweight and fast
)

pip install langchain faiss-cpu sentence-transformers

vectorstore = FAISS.from_documents(chunks, embedding_model)
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

from langchain.prompts import (
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    ChatPromptTemplate,
)

system_template = """
You are a TB awareness assistant.
Answer ONLY using the information in the provided source document chunks.
- Always cite the source chunk id (metadata 'source').
- If the answer is not in the document, say: "I cannot find that in the manual; please refer to the local PHC or call 1800-11-6666."
- Do not make up or assume anything outside the document.

Examples (do NOT invent answers):
Q: Who is Sachin Tendulkar?
A: I cannot find that in the manual; please refer to the local PHC or call 1800-11-6666.

Q: What are TB symptoms?
A: The main symptoms are: persistent cough for 2 weeks or more, fever, night sweats, weight loss, fatigue. Source: chunk_12
"""

human_template = """
Use the following document snippets to answer the question.
If snippets do not contain the answer, reply: "I cannot find that in the manual; please refer to the local PHC or call 1800-11-6666."

--- Retrieved snippets:
{context}

--- Question:
{question}
"""

system_msg = SystemMessagePromptTemplate.from_template(system_template)
human_msg = HumanMessagePromptTemplate.from_template(human_template)
chat_prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])

from langchain.chains import RetrievalQA

system_instructions = """
You are TB-AwareGPT, an assistant for NGO volunteers.
Answer using ONLY the provided manual excerpts. Do NOT provide medical advice beyond the manual.
If the question is not related to TB, please say "Question not related to TB, please ask question related to TB"
When volunteers prefer Hindi, produce answers in Hindi. Keep replies short and actionable.
"""

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": chat_prompt},
)

def answer_query(user_name, question, lang="en"):
    # get retrieved docs and answer
    result = qa_chain({"query": question})
    answer = result["result"]  # the LLM-produced answer
    # Localization
    if lang.lower().startswith("hi"):
        # ask LLM to translate/compose in Hindi, but still constrained
        prompt = f"Translate and adapt to conversational Hindi the following factual answer (do not add new info):\n\n{answer}"
        answer_hi = llm(prompt)
        return answer_hi.content
    return answer

def generate_quiz(section_text, n_questions=3, lang="en"):
    prompt = f"""
    Create {n_questions} short quiz items (question, three options, correct option, one-line explanation)
    based ONLY on the following text: {section_text}
    Output as JSON list: [{{"q":"", "opts":["","", ""], "a":0, "ex":"..."}}, ...]
    """
    resp = llm(prompt)
    # parse resp.content into structured quiz (use json.loads after ensuring valid JSON)
    return resp.content

def grade_answer(quiz_item, given_answer_index):
    correct = quiz_item["a"]
    if given_answer_index == correct:
        return True, "Correct: " + quiz_item["ex"]
    return False, f"Incorrect. Correct: option {correct+1}. {quiz_item['ex']}"

query = "What are the common symptoms of TB I should tell people about?"
result = qa_chain.invoke({"query": query})
print("Answer:", result["result"])

query = "Who is Sachin Tendulkar"
result = qa_chain.invoke({"query": query})
print("Answer:", result["result"])

